{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cnUBUiqqqDB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "import torchvision.models as models\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import numpy as np\n",
        "import gc\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWws3OIeqx78",
        "outputId": "d2efd735-dc01-410f-d263-2e54d62760e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded file size: 11635930\n",
            "First 4 bytes (should be 'PAR1'): b'PAR1'\n"
          ]
        }
      ],
      "source": [
        "import gdown\n",
        "import zipfile\n",
        "\n",
        "# === Google Drive File IDs ===\n",
        "EMBEDDINGS_FILE_ID = \"1k_ua1tHAWKLJQt_89oFUy-yZVum1onPr\"\n",
        "KEYWORD_FILE_ID = \"1VdVOWPoSk40Yucacg2WQNuuKNLqrrlD3\"\n",
        "TRIPLETS_FILE_ID = \"1aZoyVQ0rsn8yQPM36tfTa_5sjfLKi8Zb\"\n",
        "PATCH_ZIP_ID = \"1JA3aoGyhyKbCIn4uikyKOAoN0X_T_D5d\"\n",
        "\n",
        "# === Config ===\n",
        "EMBED_DIM = 128\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "\n",
        "# === Download parquet file from Google Drive ===\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "parquet_path = \"data/terrain_embeddings.parquet\"\n",
        "if not os.path.exists(parquet_path):\n",
        "    gdown.download(f\"https://drive.google.com/uc?id={EMBEDDINGS_FILE_ID}\", parquet_path, quiet=False)\n",
        "\n",
        "# === Download keyword vectors from Google Drive ===\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "keyword_path = \"data/keyword_vecs.pt\"\n",
        "if not os.path.exists(keyword_path):\n",
        "    gdown.download(f\"https://drive.google.com/uc?id={KEYWORD_FILE_ID}\", keyword_path, quiet=False)\n",
        "\n",
        "# === Download contrastive learning triplets from Google Drive ===\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "triplets_path = \"data/triplets.pt\"\n",
        "if not os.path.exists(triplets_path):\n",
        "    gdown.download(f\"https://drive.google.com/uc?id={TRIPLETS_FILE_ID}\", triplets_path, quiet=False)\n",
        "\n",
        "# === Download and unzip terrain patches ===\n",
        "PATCH_DIR = \"data/terrain_patches\"\n",
        "patch_zip_path = \"data/terrain_patches.zip\"\n",
        "if not os.path.exists(PATCH_DIR):\n",
        "    gdown.download(f\"https://drive.google.com/uc?id={PATCH_ZIP_ID}\", patch_zip_path, quiet=False)\n",
        "    with zipfile.ZipFile(patch_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"data\")\n",
        "\n",
        "\n",
        "df = pd.read_parquet(parquet_path)\n",
        "triplets = torch.load(triplets_path)\n",
        "keyword_vecs = torch.load(keyword_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UNEqEIn7q0Ic"
      },
      "outputs": [],
      "source": [
        "# === Filter None values ===\n",
        "def safe_collate(batch):\n",
        "    batch = [b for b in batch if b is not None]\n",
        "    return default_collate(batch) if batch else None\n",
        "\n",
        "# === Text Encoder ===\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, keyword_vecs):\n",
        "        super().__init__()\n",
        "        self.keyword_vecs = keyword_vecs.to(DEVICE)\n",
        "        self.proj = nn.Linear(self.keyword_vecs.shape[1], EMBED_DIM)\n",
        "\n",
        "    def forward(self, indices):\n",
        "        vecs = self.keyword_vecs[indices]\n",
        "        return self.proj(vecs)\n",
        "\n",
        "# === Geo Encoder ===\n",
        "class GeoEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(2, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, EMBED_DIM)\n",
        "        )\n",
        "\n",
        "    def forward(self, coords):\n",
        "        return self.mlp(coords.to(DEVICE))\n",
        "\n",
        "# === Terrain Encoder ===\n",
        "class TerrainEncoder(nn.Module):\n",
        "    def __init__(self, dim=128):\n",
        "        super().__init__()\n",
        "        base = models.resnet18(pretrained=True)\n",
        "\n",
        "        weight = base.conv1.weight\n",
        "        new_weight = weight.sum(dim=1, keepdim=True) / 3.0\n",
        "        base.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        base.conv1.weight.data = new_weight\n",
        "\n",
        "        self.encoder = nn.Sequential(*(list(base.children())[:-1]))\n",
        "        self.proj = nn.Linear(512, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x).squeeze(-1).squeeze(-1)\n",
        "        x = self.proj(x)\n",
        "        return F.normalize(x, dim=-1)\n",
        "\n",
        "# === Fused Encoder ===\n",
        "class FusedEncoder(nn.Module):\n",
        "    def __init__(self, keyword_vecs):\n",
        "        super().__init__()\n",
        "        self.text_encoder = TextEncoder(keyword_vecs)\n",
        "        self.geo_encoder = GeoEncoder()\n",
        "        self.terrain_encoder = TerrainEncoder()\n",
        "        self.fuse = nn.Sequential(\n",
        "            nn.Linear(3 * EMBED_DIM, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, EMBED_DIM)\n",
        "        )\n",
        "\n",
        "    def forward(self, texts, coords, terrain):\n",
        "        text_emb = self.text_encoder(texts)\n",
        "        geo_emb = self.geo_encoder(coords)\n",
        "        terrain_emb = self.terrain_encoder(terrain)\n",
        "        return self.fuse(torch.cat([text_emb, geo_emb, terrain_emb], dim=-1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V65CizB1D0e_"
      },
      "source": [
        "### Preloaded dataset for contrastive learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "I0sO-UrRD2p-"
      },
      "outputs": [],
      "source": [
        "class TripletDataset(Dataset):\n",
        "    def __init__(self, df, triplets):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.triplets = triplets\n",
        "\n",
        "    def load_patch(self, patch_id):\n",
        "        path = os.path.join(\"data/terrain_patches\", f\"{patch_id}.npy\")\n",
        "        patch = np.load(path, mmap_mode='r')\n",
        "        return torch.tensor(patch).unsqueeze(0).float() / 1000.0\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        a_idx, p_idx, n_idx = self.triplets[idx]\n",
        "\n",
        "        def get_item(i):\n",
        "            row = self.df.iloc[i]\n",
        "            patch = self.load_patch(row[\"terrain_patch_id\"])\n",
        "            coord = torch.tensor([row[\"lat\"], row[\"lon\"]], dtype=torch.float32)\n",
        "            return patch, i, coord\n",
        "\n",
        "        return get_item(a_idx) + get_item(p_idx) + get_item(n_idx)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.triplets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OGhUPfbqrNUZ"
      },
      "outputs": [],
      "source": [
        "# === InfoNCE Loss ===\n",
        "def info_nce(anchor, positive, negative, temperature=0.07):\n",
        "    anchor = F.normalize(anchor, dim=-1)\n",
        "    positive = F.normalize(positive, dim=-1)\n",
        "    negative = F.normalize(negative, dim=-1)\n",
        "    pos_sim = torch.exp(torch.sum(anchor * positive, dim=-1) / temperature)\n",
        "    neg_sim = torch.exp(torch.sum(anchor * negative, dim=-1) / temperature)\n",
        "    return -torch.log(pos_sim / (pos_sim + neg_sim)).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNF4YZODrRBr",
        "outputId": "a71147f7-6d2c-49d3-e063-1c5c6ab30908"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 172MB/s]\n",
            "Epoch 1/10: 100%|██████████| 524/524 [27:24<00:00,  3.14s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Loss = 0.1442\n",
            "Saved checkpoint to checkpoints/fused_encoder_epoch_1.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10: 100%|██████████| 524/524 [27:38<00:00,  3.16s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Loss = 0.0493\n",
            "Saved checkpoint to checkpoints/fused_encoder_epoch_2.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10: 100%|██████████| 524/524 [27:31<00:00,  3.15s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Loss = 0.0439\n",
            "Saved checkpoint to checkpoints/fused_encoder_epoch_3.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10: 100%|██████████| 524/524 [27:32<00:00,  3.15s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: Loss = 0.0448\n",
            "Saved checkpoint to checkpoints/fused_encoder_epoch_4.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10: 100%|██████████| 524/524 [27:29<00:00,  3.15s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: Loss = 0.0413\n",
            "Saved checkpoint to checkpoints/fused_encoder_epoch_5.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/10:  58%|█████▊    | 305/524 [16:01<11:59,  3.28s/it]"
          ]
        }
      ],
      "source": [
        "CHECKPOINT_DIR = \"checkpoints\"\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "model = FusedEncoder(keyword_vecs).to(DEVICE)\n",
        "dataset = TripletDataset(df, triplets)\n",
        "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
        "        if batch is None:\n",
        "            continue\n",
        "        (anchor_patch, anchor_text_idx, anchor_coord,\n",
        "         pos_patch, pos_text_idx, pos_coord,\n",
        "         neg_patch, neg_text_idx, neg_coord) = batch\n",
        "\n",
        "        anchor_patch, pos_patch, neg_patch = anchor_patch.to(DEVICE), pos_patch.to(DEVICE), neg_patch.to(DEVICE)\n",
        "        anchor_coord, pos_coord, neg_coord = anchor_coord.to(DEVICE), pos_coord.to(DEVICE), neg_coord.to(DEVICE)\n",
        "\n",
        "        a = model(anchor_text_idx.long().to(DEVICE), anchor_coord, anchor_patch)\n",
        "        p = model(pos_text_idx.long().to(DEVICE), pos_coord, pos_patch)\n",
        "        n = model(neg_text_idx.long().to(DEVICE), neg_coord, neg_patch)\n",
        "\n",
        "        loss = info_nce(a, p, n)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        del anchor_patch, anchor_text_idx, anchor_coord\n",
        "        del pos_patch, pos_text_idx, pos_coord\n",
        "        del neg_patch, neg_text_idx, neg_coord\n",
        "        del a, p, n, loss\n",
        "        gc.collect()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Loss = {total_loss / len(loader):.4f}\")\n",
        "\n",
        "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"fused_encoder_epoch_{epoch+1}.pt\")\n",
        "    torch.save(model.state_dict(), checkpoint_path)\n",
        "    print(f\"Saved checkpoint to {checkpoint_path}\")\n",
        "\n",
        "torch.save(model.state_dict(), \"trained_fused_encoder.pt\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
